{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from types import SimpleNamespace\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import debug as tf_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Convert data to TFRecord binary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _data_path(data_directory: str, name: str) -> str:\n",
    "    \"\"\"Construct a full path to a TFRecord file to be stored in the \n",
    "    data_directory. Will also ensure the data directory exists\n",
    "\n",
    "    Args:\n",
    "        data_directory: The directory where the records will be stored\n",
    "        name:           The name of the TFRecord\n",
    "\n",
    "    Returns:\n",
    "        The full path to the TFRecord file\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    return os.path.join(data_directory, '{}.tfrecords'.format(name))\n",
    "\n",
    "\n",
    "def _int64_feature(value: int) -> tf.train.Features.FeatureEntry:\n",
    "    \"\"\"Create a Int64List Feature\n",
    "\n",
    "    Args:\n",
    "        value: The value to store in the feature\n",
    "\n",
    "    Returns:\n",
    "        The FeatureEntry\n",
    "    \"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def _bytes_feature(value: str) -> tf.train.Features.FeatureEntry:\n",
    "    \"\"\"Create a BytesList Feature\n",
    "\n",
    "    Args:\n",
    "        value: The value to store in the feature\n",
    "\n",
    "    Returns:\n",
    "        The FeatureEntry\n",
    "    \"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def convert_to(data_set, name: str, data_directory: str, num_shards: int=1):\n",
    "    \"\"\"Convert the dataset into TFRecords on disk\n",
    "\n",
    "    Args:\n",
    "        data_set:       The MNIST data set to convert\n",
    "        name:           The name of the data set\n",
    "        data_directory: The directory where records will be stored\n",
    "        num_shards:     The number of files on disk to separate records into\n",
    "    \"\"\"\n",
    "\n",
    "    num_examples, rows, cols, depth = data_set.images.shape\n",
    "\n",
    "    data_set = list(zip(data_set.images, data_set.labels))\n",
    "\n",
    "    def _process_examples(example_dataset, filename: str):\n",
    "        print('Processing {} data'.format(filename))\n",
    "        dataset_length = len(example_dataset)\n",
    "        with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "            for index, (image, label) in enumerate(example_dataset):\n",
    "                sys.stdout.write('\\rProcessing sample {} of {}'.format(\n",
    "                    index + 1, dataset_length))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                image_raw = image.tostring()\n",
    "                example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                    'height': _int64_feature(rows),\n",
    "                    'width': _int64_feature(cols),\n",
    "                    'depth': _int64_feature(depth),\n",
    "                    'label': _int64_feature(int(label)),\n",
    "                    'image_raw': _bytes_feature(image_raw)\n",
    "                }))\n",
    "                writer.write(example.SerializeToString())\n",
    "            print()\n",
    "\n",
    "    if num_shards == 1:\n",
    "        _process_examples(data_set, _data_path(data_directory, name))\n",
    "    else:\n",
    "        sharded_dataset = np.array_split(data_set, num_shards)\n",
    "        for shard, dataset in enumerate(sharded_dataset):\n",
    "            _process_examples(dataset, _data_path(\n",
    "                data_directory, '{}-{}'.format(name, shard + 1)))\n",
    "\n",
    "\n",
    "def convert_to_tf_record(args):\n",
    "    \"\"\"Convert the TF MNIST Dataset to TFRecord formats\n",
    "\n",
    "    Args:\n",
    "        data_directory: The directory where the TFRecord files should be stored\n",
    "    \"\"\"\n",
    "\n",
    "    mnist = input_data.read_data_sets(\n",
    "        \"/tmp/tensorflow/mnist/input_data\",\n",
    "        reshape=False\n",
    "    )\n",
    "\n",
    "    convert_to(mnist.validation, 'validation', args.data_directory)\n",
    "    convert_to(mnist.train, 'train', args.data_directory, num_shards=10)\n",
    "    convert_to(mnist.test, 'test', args.data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_args = SimpleNamespace(\n",
    "    data_directory=os.path.expanduser('~/data/mnist')\n",
    ")\n",
    "\n",
    "convert_to_tf_records(prep_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data input\n",
    "\n",
    "Setup our data `input_fn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_columns(with_label=True):\n",
    "    features = {\n",
    "        'image_raw': tf.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    if with_label:\n",
    "        features['label'] = tf.FixedLenFeature([], tf.int64)\n",
    "    return features\n",
    "\n",
    "\n",
    "def make_input_fn(filenames, batch_size=1024, shuffle=False):\n",
    "\n",
    "    def _parser(record):\n",
    "        image = tf.decode_raw(record['image_raw'], tf.float32)\n",
    "\n",
    "        label = tf.cast(record['label'], tf.int32)\n",
    "\n",
    "        return {\n",
    "            'image': image\n",
    "        }, label\n",
    "\n",
    "    def _input_fn():\n",
    "        dataset = tf.contrib.data.make_batched_features_dataset(\n",
    "            file_pattern=filenames, batch_size=batch_size, \n",
    "            features=get_feature_columns(),\n",
    "            shuffle=shuffle, sloppy_ordering=True\n",
    "        )\n",
    "\n",
    "        dataset = dataset.map(_parser, num_parallel_calls=os.cpu_count())\n",
    "        return dataset\n",
    "\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "Define our CNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    \"\"\"CNN architecture to process 28x28x1 MNIST images\n",
    "    Arguments:\n",
    "        features: tensor of MNIST images\n",
    "        mode: estimator mode\n",
    "        params: dictionary of hyperparameters\n",
    "\n",
    "    Returns:\n",
    "        Tensor of the final layer output without activation\n",
    "    \"\"\"\n",
    "\n",
    "    is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    with tf.name_scope('Input'):\n",
    "        # Input Layer\n",
    "        input_layer = tf.reshape(\n",
    "            features['image'], [-1, 28, 28, 1], name='input_reshape')\n",
    "        tf.summary.image('input', input_layer)\n",
    "\n",
    "    with tf.name_scope('Conv_1'):\n",
    "        # Convolutional Layer #1\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            inputs=input_layer,\n",
    "            filters=32,\n",
    "            kernel_size=(5, 5),\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu,\n",
    "            trainable=is_training)\n",
    "\n",
    "        # Pooling Layer #1\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            inputs=conv1, pool_size=(2, 2), strides=2, padding='same')\n",
    "\n",
    "    with tf.name_scope('Conv_2'):\n",
    "        # Convolutional Layer #2 and Pooling Layer #2\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=64,\n",
    "            kernel_size=(5, 5),\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu,\n",
    "            trainable=is_training)\n",
    "\n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            inputs=conv2, pool_size=(2, 2), strides=2, padding='same')\n",
    "\n",
    "    with tf.name_scope('Dense_Dropout'):\n",
    "        # Dense Layer\n",
    "        pool2_flat = tf.layers.flatten(pool2)\n",
    "        dense = tf.layers.dense(\n",
    "            inputs=pool2_flat, units=1024, activation=tf.nn.relu, trainable=is_training)\n",
    "        dropout = tf.layers.dropout(\n",
    "            inputs=dense, rate=params['dropout_rate'], training=is_training)\n",
    "\n",
    "    with tf.name_scope('Predictions'):\n",
    "        # Logits Layer\n",
    "        logits = tf.layers.dense(\n",
    "            inputs=dropout, units=10, trainable=is_training)\n",
    "\n",
    "    head = tf.contrib.estimator.multi_class_head(\n",
    "        n_classes=10)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])\n",
    "    return head.create_estimator_spec(\n",
    "        features, mode, logits, labels, optimizer=optimizer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_spec(args, hooks):\n",
    "    train_files = os.path.join(args.data_directory, 'train-*.tfrecords')\n",
    "    \n",
    "    train_input_fn = make_input_fn(\n",
    "        train_files, batch_size=args.batch_size)\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=train_input_fn, max_steps=args.max_steps, hooks=hooks)\n",
    "    \n",
    "    return train_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_spec(args):\n",
    "    eval_files = os.path.join(args.data_directory, 'validation.tfrecords')\n",
    "    \n",
    "    eval_input_fn = make_input_fn(eval_files, batch_size=1)\n",
    "    \n",
    "    eval_spec = tf.estimator.EvalSpec(eval_input_fn)\n",
    "    \n",
    "    return eval_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    \"\"\"Run training and evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    run_config = tf.estimator.RunConfig()\n",
    "\n",
    "    hparams = {\n",
    "        'learning_rate': args.learning_rate,\n",
    "        'dropout_rate': 0.4\n",
    "    }\n",
    "\n",
    "    mnist_classifier = tf.estimator.Estimator(\n",
    "        model_fn=model_fn,\n",
    "        model_dir=args.model_directory,\n",
    "        config=run_config,\n",
    "        params=hparams\n",
    "    )\n",
    "\n",
    "    hooks = []\n",
    "\n",
    "    if args.debug_port is not None:\n",
    "        debug_hook = tf_debug.TensorBoardDebugHook(\n",
    "            \"localhost:{}\".format(args.debug_port))\n",
    "        hooks.append(debug_hook)\n",
    "\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        mnist_classifier, get_train_spec(args, hooks), get_eval_spec(args))\n",
    "    \n",
    "    return mnist_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    model_directory='',\n",
    "    data_directory='',\n",
    "    learning_rate=0.4,\n",
    "    batch_size=1024,\n",
    "    max_steps=400,\n",
    "    debug_port=None\n",
    ")\n",
    "\n",
    "classifier = train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    \n",
    "    feature_spec = {\n",
    "        'image': tf.FixedLenFeature([], dtype=tf.string)\n",
    "    }\n",
    "    \n",
    "    default_batch_size = None # the number of query examples expected per batch. Leave unset for variable batch size (recommended).\n",
    "    \n",
    "    serialized_tf_example = tf.placeholder(\n",
    "        dtype=tf.string, shape=[default_batch_size], \n",
    "        name='input_image_tensor')\n",
    "    \n",
    "    received_tensors = { 'images': serialized_tf_example }\n",
    "    features = tf.parse_example(serialized_tf_example, feature_spec)\n",
    "    \n",
    "    def map_input(image_raw):\n",
    "        image_decoded = tf.image.decode_jpeg(image_raw, channels=1)\n",
    "        # Convert from full range of uint8 to range [0,1] of float32.\n",
    "        image_decoded_as_float = tf.image.convert_image_dtype(image_decoded, dtype=tf.float32)\n",
    "        # Resize to expected\n",
    "        image_resized = tf.image.resize_images(image_decoded_as_float, size=(28, 28))\n",
    "        \n",
    "        return image_resized\n",
    "    \n",
    "    features['image'] = tf.map_fn(map_input, features['image'], dtype=tf.float32)\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(features, received_tensors)\n",
    "\n",
    "classifier.export_savedmodel('mnist-exports', serving_input_receiver_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
